{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using Keras & Scikit-Optimize: Sonar Datasets\n",
    "\n",
    "\n",
    "Hyper-parameter turning __[Scikit-Optimize](https://scikit-optimize.github.io/index.html)__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "## Importing required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sonar.csv', header=None)\n",
    "X_test = df.sample(frac=0.20, replace=True, random_state=1)\n",
    "Y_test = X_test[60]\n",
    "X_test.drop([60],axis=1, inplace=True)\n",
    "X_test.head()\n",
    "X_train = df.drop(X_test.index)\n",
    "Y_train = X_train[60]\n",
    "X_train.drop([60],axis=1, inplace=True)\n",
    "X_train.head()\n",
    "#X_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "print(\"X_test shape: \",X_test.shape)\n",
    "print(\"X_train shape: \",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the features\n",
    "for i in X_train.columns:\n",
    "    # Draw the density plot\n",
    "    sns.distplot(X_train[i], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 1})\n",
    "    \n",
    "plt.title('Density Plot for Sonar Features in Training Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the features\n",
    "for i in X_test.columns:\n",
    "    # Draw the density plot\n",
    "    sns.distplot(X_test[i], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 1})\n",
    "    \n",
    "plt.title('Density Plot for Sonar Features in Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some sample images along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%Y_train.iloc[i])\n",
    "    plt.imshow(X_train.iloc[i].values.reshape(4,15),cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T19:48:49.676086-05:00",
     "start_time": "2018-06-08T19:48:49.652151Z"
    }
   },
   "outputs": [],
   "source": [
    "## Changing labels to one-hot encoded vector\n",
    "lb = LabelBinarizer()\n",
    "y_train_one_hot = lb.fit_transform(Y_train)\n",
    "y_test_one_hot = lb.transform(Y_test)\n",
    "print('Train labels dimension:');print(y_train_one_hot.shape)\n",
    "print('Test labels dimension:');print(y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have processed the data, let's start building our multi-layer perceptron using tensorflow. We will begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T19:53:33.413023-05:00",
     "start_time": "2018-06-08T19:53:20.057677Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & Compile Keras Model for Hyper-Parameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:01:07.682313-05:00",
     "start_time": "2018-06-08T20:01:07.677315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining various initialization parameters for MLP model\n",
    "num_features = X_train.shape[1]; num_classes = y_train_one_hot.shape[1] \n",
    "\n",
    "# Let's create a helper function first which builds the model with various parameters.\n",
    "def get_model(dense_0_neurons, dense_1_neurons, dropout_rate, input_dim, num_classes):\n",
    "    # Builds a Sequential MLP model using Keras and returns it\n",
    "    \n",
    "    # Define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_0_neurons, input_dim=input_dim, activation='relu', name=\"dense_1\"))\n",
    "    model.add(Dense(dense_1_neurons, activation='relu', name=\"dense_2\"))\n",
    "    model.add(Dropout(dropout_rate, name=\"dropout\"))\n",
    "    model.add(Dense(num_classes, activation='sigmoid', name=\"dense_3\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Keras Model for Scikit-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:11:09.679237-05:00",
     "start_time": "2018-06-08T20:11:09.662284Z"
    }
   },
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# Specify `Static' Parameters\n",
    "STATIC_PARAMS = {num_features, num_classes}\n",
    "\n",
    "# Bounded region of parameter space\n",
    "# The list of hyper-parameters we want to optimize. For each one we define the\n",
    "# bounds, the corresponding scikit-learn parameter name, as well as how to\n",
    "# sample values from that dimension (`'log-uniform'` for the dropout_rate)\n",
    "SPACE = [skopt.space.Integer(8, 512, name='dense_0_neurons'),\n",
    "         skopt.space.Integer(16, 512, name='dense_1_neurons'),\n",
    "        skopt.space.Real(0.0, 0.6, name='dropout_rate')]\n",
    "\n",
    "# This decorator allows your objective function to receive a the parameters as\n",
    "# keyword arguments. This is particularly convenient when you want to set\n",
    "# scikit-learn estimator parameters         \n",
    "@skopt.utils.use_named_args(SPACE)\n",
    "         \n",
    "# Define objective for optimization\n",
    "def objective(**params):\n",
    "  \n",
    "    # All parameters: \n",
    "    #all_params = {**params, **STATIC_PARAMS}\n",
    "    \n",
    "    # Create the model using a specified hyperparameters.\n",
    "    #model = get_model(all_params)\n",
    "    model = get_model(params[\"dense_0_neurons\"], params[\"dense_1_neurons\"], params[\"dropout_rate\"], num_features, num_classes)\n",
    "\n",
    "    # Compile the keras model for a specified number of epochs.\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit keras model\n",
    "    history = model.fit(X_train, y_train_one_hot, epochs=1, batch_size=8, \n",
    "                        validation_split = 0.20, verbose=1)\n",
    "\n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(X_test, y_test_one_hot,\n",
    "                                  batch_size=8, verbose=1)\n",
    "    print('Test loss:', score[0], '   Test accuracy:', score[1])\n",
    "\n",
    "    # Return the accuracy.\n",
    "    return -1.0 * score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scikit-Optimizer\n",
    "There are several methods for optimization: https://scikit-optimize.github.io/modules/minimize_functions.html#minimize-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results_gp = skopt.gbrt_minimize(objective,     # the function to minimize\n",
    "                          SPACE,              # the bounds on each dimension of x\n",
    "                          x0=[100,50,0],      # the starting point\n",
    "                          acq_func=\"LCB\",     # the acquisition function (optional)\n",
    "                          n_calls=50,         # the number of evaluations of f including at x0\n",
    "                          n_random_starts=4,  # the number of random initialization points\n",
    "                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Best Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score=%.4f\" % results_gp.fun)\n",
    "\n",
    "print(\"\"\"Best parameters:\n",
    "- dense_0_neurons=%d\n",
    "- dense_1_neurons=%d\n",
    "- dropout_rate=%.6f\"\"\" % \n",
    "      (results_gp.x[0], results_gp.x[1],\n",
    "                            results_gp.x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(results_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations Plot\n",
    "plots.plot_evaluations creates a grid of size n_dims by n_dims. The diagonal shows histograms for each of the dimensions. In the lower triangle (just one plot in this case) a two dimensional scatter plot of all points is shown. The order in which points were evaluated is encoded in the color of each point. Darker/purple colors correspond to earlier samples and lighter/yellow colors correspond to later samples. A red point shows the location of the minimum found by the optimization process.\n",
    "\n",
    "You should be able to see that points start clustering around the location of the true miminum. The histograms show that the objective is evaluated more often at locations near to one of the three minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_evaluations\n",
    "_ = plot_evaluations(results_gp, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Partial Dependence Plots\n",
    "Partial dependence plots were proposed by [Friedman (2001)] as a method for interpreting the importance of input features used in gradient boosting machines. The idea is to visualize how the value of $i$-th variable $x_i$ influences the function $f$ after averaging out the influence of all other variables.For more details see: https://christophm.github.io/interpretable-ml-book/pdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "_ = plot_objective(results_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Further using Best Hyper-parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using a specified hyperparameters.\n",
    "\n",
    "# Get best hyper-parameters\n",
    "dense_0_neurons=results_gp.x[0]; dense_1_neurons=results_gp.x[1]; dropout_rate=results_gp.x[2]\n",
    "\n",
    "# Build \n",
    "model = get_model(dense_0_neurons, dense_1_neurons, dropout_rate, num_features, num_classes)\n",
    "\n",
    "# Compile the keras model for a specified number of epochs.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fit keras model\n",
    "history = model.fit(X_train, y_train_one_hot, epochs=16, batch_size=8, \n",
    "                        validation_split = 0.20, verbose=1)\n",
    "\n",
    "# Evaluate the model with the eval dataset.\n",
    "score = model.evaluate(X_test, y_test_one_hot,\n",
    "                                  batch_size=8, verbose=1)\n",
    "print('Test loss:', score[0], '   Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Training History\n",
    "In this section, we will produce plots of your model's accuracy and loss on the training and validation set. These are useful to check for overfitting. Additionally, you can produce these plots for any of the metrics you created above. False negatives are included as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for plotting history\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'acc']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(1,2,n+1)\n",
    "    plt.tight_layout()\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training/validation history of our Keras model\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Metrics\n",
    "You can use a confusion matrix to summarize the actual vs. predicted labels where the X axis is the predicted label and the Y axis is the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_cm(labels, predictions):\n",
    "  cm = confusion_matrix(labels, predictions)\n",
    "  plt.figure(figsize=(8,8))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model on the test dataset and display the evaluation metrics and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = model.predict_classes(X_test, batch_size=8)\n",
    "baseline_results = model.evaluate(X_test, y_test_one_hot,\n",
    "                                  batch_size=8, verbose=0)\n",
    "\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(Y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see we have successfully trained a Multi-Layer perceptron which was written in Keras using TensorFlow backend with high validation accuracy!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
