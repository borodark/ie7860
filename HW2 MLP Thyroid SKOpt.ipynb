{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using Keras & Scikit-Optimize: Sonar an Thyroid Datasets\n",
    "##### Partial Sources: Scikit-Optimize __[Source](https://scikit-optimize.github.io/auto_examples/visualizing-results.html)__ | Aayush Agrawal  __[Source](https://github.com/aayushmnit/Deep_learning_explorations)__\n",
    "\n",
    "In this notebook, we are going to build an MLP neural network using `Keras`, `Tensorflow` and successfully train it to recognize digits in the very popular __[MNIST](https://en.wikipedia.org/wiki/MNIST_database)__ dataset images. \n",
    "\n",
    "__[Keras](https://keras.io)__ is a high-level neural networks API, written in Python and capable of running on top of __[TensorFlow](https://www.tensorflow.org)__ (developed by Google), __[CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/)__ (developed by Microsoft), or __[Theano](http://deeplearning.net/software/theano/)__. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "__[TensorFlow](https://www.tensorflow.org)__ is a very popular deep learning framework and is the default backend for Keras. \n",
    "\n",
    "This notebook is a guide for building an MLP neural network using Keras and TensorFlow backend.\n",
    "\n",
    "As for hyper-parameter turning, we shall rely on __[Scikit-Optimize](https://scikit-optimize.github.io/index.html)__.\n",
    "\n",
    "Let's start by importing our data. As keras, a high-level deep learning library already has MNIST as part of their default data we are just going to import the dataset from there and split it into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5   6   7   8   9   ...  11  12  13  14  15       16  \\\n",
       "0  0.73   0   1   0   0   0   0   0   1   0  ...   0   0   0   0   0  0.00060   \n",
       "1  0.24   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0  0.00025   \n",
       "2  0.47   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0  0.00190   \n",
       "3  0.64   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0  0.00090   \n",
       "4  0.23   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0  0.00025   \n",
       "\n",
       "      17     18     19     20  \n",
       "0  0.015  0.120  0.082  0.146  \n",
       "1  0.030  0.143  0.133  0.108  \n",
       "2  0.024  0.102  0.131  0.078  \n",
       "3  0.017  0.077  0.090  0.085  \n",
       "4  0.026  0.139  0.090  0.153  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "## Importing required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_cm(labels, predictions):\n",
    "  cm = confusion_matrix(labels, predictions)\n",
    "  names = np.unique(labels)\n",
    "  #print() \n",
    "  df_cm = pd.DataFrame(cm, names,names)\n",
    "  print(cm)\n",
    "  plt.figure(figsize=(8,8))\n",
    "  sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "import pandas as pd\n",
    "X_train = pd.read_csv('ann-train.data', header=None, sep=\" \") \n",
    "Y_train=X_train[21]\n",
    "X_train.drop([21,22,23],axis=1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes -> number of representatives:\n",
      "3    3488\n",
      "2     191\n",
      "1      93\n",
      "Name: 21, dtype: int64\n",
      "Times more needed for class 1 ->  37\n",
      "Times more needed for class 2 ->  18\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes -> number of representatives:\")\n",
    "print(Y_train.value_counts())\n",
    "times_class1 = Y_train.value_counts()[3]//Y_train.value_counts()[1]\n",
    "times_class2 = Y_train.value_counts()[3]//Y_train.value_counts()[2]\n",
    "print(\"Times more needed for class 1 -> \", times_class1)\n",
    "print(\"Times more needed for class 2 -> \", times_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:  (93, 21)\n",
      "Class 2:  (191, 21)\n",
      "Shape of padded class 1 with label:  (3348, 22)\n",
      "Shape of padded class 2 with label:  (3247, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.09800</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00979</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00820</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.07000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00630</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.06500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.07692</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5   6   7   8   9   ...  12  13  14  15       16  \\\n",
       "0  0.62   0   0   0   0   1   0   0   0   0  ...   0   0   0   0  0.01100   \n",
       "1  0.61   0   0   0   0   1   0   0   0   0  ...   0   0   0   0  0.00979   \n",
       "2  0.56   0   0   0   1   0   0   0   0   0  ...   0   0   0   0  0.00820   \n",
       "3  0.78   0   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.00630   \n",
       "4  0.69   0   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.00680   \n",
       "\n",
       "      17     18     19       20  21  \n",
       "0  0.008  0.073  0.074  0.09800   2  \n",
       "1  0.004  0.081  0.064  0.12600   2  \n",
       "2  0.020  0.066  0.094  0.07000   2  \n",
       "3  0.011  0.056  0.086  0.06500   2  \n",
       "4  0.022  0.077  0.100  0.07692   2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create more records for underrepresented classes\n",
    "class1indexes = Y_train.loc[Y_train == 1]\n",
    "class2indexes = Y_train.loc[Y_train == 2]\n",
    "\n",
    "#class2indexes.head()\n",
    "\n",
    "class1 = X_train.loc[class1indexes.index]\n",
    "class2 = X_train.loc[class2indexes.index]\n",
    "print(\"Class 1: \",class1.shape)\n",
    "print(\"Class 2: \",class2.shape)\n",
    "pad1 = class1.reset_index(drop=True)\n",
    "#print(pad.shape)\n",
    "pad1 = pd.concat([pad1 for i in range(times_class1-1)],\n",
    "          ignore_index=True)\n",
    "\n",
    "pad1[21] = pd.Series(1, index=pad1.index) # add class label 1\n",
    "#class1labels = \n",
    "print(\"Shape of padded class 1 with label: \", pad1.shape)\n",
    "#pad1.head()\n",
    "pad2 = class2.reset_index(drop=True)\n",
    "pad2 = pd.concat([pad2 for i in range(times_class2-1)],\n",
    "          ignore_index=True)\n",
    "\n",
    "pad2[21] = pd.Series(2, index=pad2.index) # add class label 1\n",
    "print(\"Shape of padded class 2 with label: \", pad2.shape)\n",
    "pad2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (3772, 21)\n",
      "Training data shape with yZ:  (3772, 22)\n",
      "New Training data shape:  (10367, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.0730</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   ...  12  13  14  15      16  \\\n",
       "7594  0.25   0   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.0264   \n",
       "4507  0.50   1   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.0800   \n",
       "4916  0.60   1   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.0980   \n",
       "1268  0.67   0   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.0510   \n",
       "6900  0.37   0   0   0   0   0   0   0   0   0  ...   0   0   0   0  0.0550   \n",
       "\n",
       "         17      18     19     20  21  \n",
       "7594  0.023  0.0730  0.109  0.067   2  \n",
       "4507  0.014  0.0410  0.114  0.036   1  \n",
       "4916  0.004  0.0058  0.080  0.007   1  \n",
       "1268  0.004  0.0120  0.126  0.010   1  \n",
       "6900  0.009  0.0450  0.104  0.043   1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training data shape: \",X_train.shape)\n",
    "X_train[21] = Y_train\n",
    "print(\"Training data shape with yZ: \",X_train.shape)\n",
    "X_train = X_train.append(pad1,ignore_index=True).append(pad2,ignore_index=True).sample(frac=1)\n",
    "#train.reset_index(inplace=True, drop=True)\n",
    "print(\"New Training data shape: \",X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yZ of training data shape:  (10367,)\n",
      "training data shape:  (10367, 21)\n",
      "Classes -> number of representatives:\n",
      "3    3488\n",
      "1    3441\n",
      "2    3438\n",
      "Name: 21, dtype: int64\n",
      "(10367, 21)\n"
     ]
    }
   ],
   "source": [
    "# keep the class labels of training dataframe\n",
    "Y_train=X_train[21]\n",
    "print(\"yZ of training data shape: \",Y_train.shape)\n",
    "# remove lables from training dataframe\n",
    "X_train.drop([21],axis=1, inplace=True)\n",
    "print(\"training data shape: \",X_train.shape)\n",
    "print(\"Classes -> number of representatives:\")\n",
    "print(Y_train.value_counts())\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some sample images along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10367, 21)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%Y_train.iloc[i])\n",
    "    plt.imshow(X_train.iloc[i].values.reshape(3,7),cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>-1.381739</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>-0.128653</td>\n",
       "      <td>0.881545</td>\n",
       "      <td>-0.143196</td>\n",
       "      <td>0.443369</td>\n",
       "      <td>-0.302785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>-0.066733</td>\n",
       "      <td>1.706913</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>0.570061</td>\n",
       "      <td>-0.241460</td>\n",
       "      <td>-0.907747</td>\n",
       "      <td>0.718464</td>\n",
       "      <td>-1.047573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>0.459269</td>\n",
       "      <td>1.706913</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>0.804704</td>\n",
       "      <td>-1.489244</td>\n",
       "      <td>-1.748754</td>\n",
       "      <td>-1.152184</td>\n",
       "      <td>-1.744310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0.827470</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>0.192026</td>\n",
       "      <td>-1.489244</td>\n",
       "      <td>-1.600622</td>\n",
       "      <td>1.378692</td>\n",
       "      <td>-1.672233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>-0.750536</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>-0.865352</td>\n",
       "      <td>-0.812178</td>\n",
       "      <td>0.168273</td>\n",
       "      <td>-0.879395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "7594 -1.381739 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "4507 -0.066733  1.706913 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "4916  0.459269  1.706913 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "1268  0.827470 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "6900 -0.750536 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "\n",
       "            7         8       9   ...        11       12        13        14  \\\n",
       "7594 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "4507 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "4916 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "1268 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "6900 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "\n",
       "            15        16        17        18        19        20  \n",
       "7594 -0.179041 -0.128653  0.881545 -0.143196  0.443369 -0.302785  \n",
       "4507 -0.179041  0.570061 -0.241460 -0.907747  0.718464 -1.047573  \n",
       "4916 -0.179041  0.804704 -1.489244 -1.748754 -1.152184 -1.744310  \n",
       "1268 -0.179041  0.192026 -1.489244 -1.600622  1.378692 -1.672233  \n",
       "6900 -0.179041  0.244168 -0.865352 -0.812178  0.168273 -0.879395  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes -> number of representatives:\n",
      "3    3178\n",
      "2     177\n",
      "1      73\n",
      "Name: 21, dtype: int64\n",
      "test data shape:  (3428, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.171338</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>-0.393278</td>\n",
       "      <td>1.505437</td>\n",
       "      <td>0.764710</td>\n",
       "      <td>1.653788</td>\n",
       "      <td>0.129673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.013537</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>-0.455849</td>\n",
       "      <td>0.382432</td>\n",
       "      <td>0.119619</td>\n",
       "      <td>-1.262222</td>\n",
       "      <td>0.658232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.855737</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>-0.472795</td>\n",
       "      <td>1.879772</td>\n",
       "      <td>3.822917</td>\n",
       "      <td>-0.051803</td>\n",
       "      <td>3.829586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.592140</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>-0.181891</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>-0.459760</td>\n",
       "      <td>0.257653</td>\n",
       "      <td>0.191296</td>\n",
       "      <td>-0.712032</td>\n",
       "      <td>0.466028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.539539</td>\n",
       "      <td>-0.585853</td>\n",
       "      <td>-0.287013</td>\n",
       "      <td>-0.098691</td>\n",
       "      <td>-0.076297</td>\n",
       "      <td>5.497809</td>\n",
       "      <td>-0.071684</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.132927</td>\n",
       "      <td>-0.3426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059031</td>\n",
       "      <td>-0.05651</td>\n",
       "      <td>-0.163214</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>-0.179041</td>\n",
       "      <td>-0.467581</td>\n",
       "      <td>0.756767</td>\n",
       "      <td>1.314231</td>\n",
       "      <td>1.873864</td>\n",
       "      <td>0.466028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -1.171338 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "1 -1.013537 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "2 -0.855737 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "3 -1.592140 -0.585853 -0.287013 -0.098691 -0.076297 -0.181891 -0.071684   \n",
       "4 -1.539539 -0.585853 -0.287013 -0.098691 -0.076297  5.497809 -0.071684   \n",
       "\n",
       "         7         8       9   ...        11       12        13        14  \\\n",
       "0 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "1 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "2 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "3 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "4 -0.110921 -0.132927 -0.3426  ... -0.059031 -0.05651 -0.163214 -0.009822   \n",
       "\n",
       "         15        16        17        18        19        20  \n",
       "0 -0.179041 -0.393278  1.505437  0.764710  1.653788  0.129673  \n",
       "1 -0.179041 -0.455849  0.382432  0.119619 -1.262222  0.658232  \n",
       "2 -0.179041 -0.472795  1.879772  3.822917 -0.051803  3.829586  \n",
       "3 -0.179041 -0.459760  0.257653  0.191296 -0.712032  0.466028  \n",
       "4 -0.179041 -0.467581  0.756767  1.314231  1.873864  0.466028  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv('ann-test.data', header=None, sep=\" \") \n",
    "Y_test=X_test[21]\n",
    "X_test.drop([21,22,23],axis=1, inplace=True)\n",
    "\n",
    "print(\"Classes -> number of representatives:\")\n",
    "print(Y_test.value_counts())\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "print(\"test data shape: \",X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T19:48:49.676086-05:00",
     "start_time": "2018-06-08T19:48:49.652151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels dimension:\n",
      "(10367, 3)\n",
      "Test labels dimension:\n",
      "(3428, 3)\n"
     ]
    }
   ],
   "source": [
    "## Changing labels to one-hot encoded vector\n",
    "lb = LabelBinarizer()\n",
    "y_train_one_hot = lb.fit_transform(Y_train)\n",
    "y_test_one_hot = lb.transform(Y_test)\n",
    "print('Train labels dimension:');print(y_train_one_hot.shape)\n",
    "print('Test labels dimension:');print(y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have processed the data, let's start building our multi-layer perceptron using tensorflow. We will begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T19:53:33.413023-05:00",
     "start_time": "2018-06-08T19:53:20.057677Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & Compile Keras Model for Hyper-Parameter Optimization\n",
    "Models in Keras are defined as a sequence of layers. We create a `Sequential` model and add layers one at a time until we are happy with our network architecture.\n",
    "\n",
    "The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the input_dim argument and setting it to 784 for the 784 input pixels.\n",
    "\n",
    "In this example, we will use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers are defined using the `Dense` class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the activation argument.\n",
    "\n",
    "We will use the rectified linear unit activation function referred to as `ReLU` on the first two layers and the `Sigmoid` function in the output layer.\n",
    "\n",
    "We can piece it all together by adding each layer:\n",
    "The model expects rows of data with 784 variables (the input_dim=784 argument)\n",
    "The first hidden layer has num_layers_0 = 512 nodes and uses the relu activation function.\n",
    "The second hidden layer has num_layers_1 = 256 nodes and uses the relu activation function.\n",
    "The output layer has 10 nodes for the ten digits/classes.\n",
    "\n",
    "Let's start our model construction by defining initialization variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:01:07.682313-05:00",
     "start_time": "2018-06-08T20:01:07.677315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining various initialization parameters for MLP model\n",
    "num_features = X_train.shape[1]; num_classes = y_train_one_hot.shape[1] \n",
    "\n",
    "# Let's create a helper function first which builds the model with various parameters.\n",
    "def get_model(dense_0_neurons, dense_1_neurons, dropout_rate, input_dim, num_classes):\n",
    "    # Builds a Sequential MLP model using Keras and returns it\n",
    "    \n",
    "    # Define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_0_neurons, input_dim=input_dim, activation='relu', name=\"dense_1\"))\n",
    "    model.add(Dense(dense_1_neurons, activation='relu', name=\"dense_2\"))\n",
    "    model.add(Dropout(dropout_rate, name=\"dropout\"))\n",
    "    model.add(Dense(num_classes, activation='sigmoid', name=\"dense_3\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Keras Model for Scikit-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:11:09.679237-05:00",
     "start_time": "2018-06-08T20:11:09.662284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3, 21}\n"
     ]
    }
   ],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# Specify `Static' Parameters\n",
    "STATIC_PARAMS = {num_features, num_classes}\n",
    "\n",
    "# Bounded region of parameter space\n",
    "# The list of hyper-parameters we want to optimize. For each one we define the\n",
    "# bounds, the corresponding scikit-learn parameter name, as well as how to\n",
    "# sample values from that dimension (`'log-uniform'` for the dropout_rate)\n",
    "SPACE = [skopt.space.Integer(8, 512, name='dense_0_neurons'),\n",
    "         skopt.space.Integer(8, 512, name='dense_1_neurons'),\n",
    "        skopt.space.Real(0.0, 0.8, name='dropout_rate')]\n",
    "\n",
    "# This decorator allows your objective function to receive a the parameters as\n",
    "# keyword arguments. This is particularly convenient when you want to set\n",
    "# scikit-learn estimator parameters         \n",
    "@skopt.utils.use_named_args(SPACE)\n",
    "         \n",
    "# Define objective for optimization\n",
    "def objective(**params):\n",
    "  \n",
    "    # All parameters: \n",
    "    #all_params = {**params, **STATIC_PARAMS}\n",
    "    \n",
    "    # Create the model using a specified hyperparameters.\n",
    "    #model = get_model(all_params)\n",
    "    model = get_model(params[\"dense_0_neurons\"], params[\"dense_1_neurons\"], params[\"dropout_rate\"], num_features, num_classes)\n",
    "\n",
    "    # Compile the keras model for a specified number of epochs.\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Fit keras model\n",
    "    history = model.fit(X_train, y_train_one_hot, epochs=8, batch_size=16, \n",
    "                        validation_split = 0.20, verbose=1)\n",
    "\n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(X_test, y_test_one_hot,\n",
    "                                  batch_size=16, verbose=1)\n",
    "    print('Test loss:', score[0], '   Test accuracy:', score[1])\n",
    "\n",
    "    # Return the accuracy.\n",
    "    return -1.0 * score[1]\n",
    "\n",
    "print(STATIC_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scikit-Optimizer\n",
    "There are several methods for optimization: https://scikit-optimize.github.io/modules/minimize_functions.html#minimize-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/iostaptchenko/projects/secret/wsu/ds/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/iostaptchenko/projects/secret/wsu/ds/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 1s 105us/step - loss: nan - categorical_accuracy: 0.7230 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 83us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 76us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 77us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 83us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 89us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 90us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 79us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 37us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "WARNING:tensorflow:From /Users/iostaptchenko/projects/secret/wsu/ds/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 229us/step - loss: nan - categorical_accuracy: 0.5923 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 166us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 175us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 161us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 153us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 154us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 178us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 155us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 49us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 235us/step - loss: nan - categorical_accuracy: 0.4325 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 179us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 176us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 162us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 187us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 179us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 173us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 177us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 47us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 1s 137us/step - loss: 0.4805 - categorical_accuracy: 0.7770 - val_loss: 0.3260 - val_categorical_accuracy: 0.8525\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 94us/step - loss: nan - categorical_accuracy: 0.8428 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 127us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 91us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 93us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 107us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 119us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 83us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 36us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 1s 180us/step - loss: 0.4149 - categorical_accuracy: 0.8012 - val_loss: 0.3115 - val_categorical_accuracy: 0.8607\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 136us/step - loss: nan - categorical_accuracy: 0.4615 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 142us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 143us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 137us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 139us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 141us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 142us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428/3428 [==============================] - 0s 43us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 1s 138us/step - loss: 0.8499 - categorical_accuracy: 0.5096 - val_loss: 0.5390 - val_categorical_accuracy: 0.6851\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 85us/step - loss: 0.7627 - categorical_accuracy: 0.5653 - val_loss: 0.5002 - val_categorical_accuracy: 0.6538\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 84us/step - loss: 0.7373 - categorical_accuracy: 0.5629 - val_loss: 0.5004 - val_categorical_accuracy: 0.6610\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 84us/step - loss: 0.7303 - categorical_accuracy: 0.5595 - val_loss: 0.4870 - val_categorical_accuracy: 0.6610\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 85us/step - loss: 0.7213 - categorical_accuracy: 0.5617 - val_loss: 0.4821 - val_categorical_accuracy: 0.6610\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 84us/step - loss: 0.7144 - categorical_accuracy: 0.5649 - val_loss: 0.4741 - val_categorical_accuracy: 0.6610\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 86us/step - loss: 0.7186 - categorical_accuracy: 0.5663 - val_loss: 0.4762 - val_categorical_accuracy: 0.6610\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 90us/step - loss: 0.7209 - categorical_accuracy: 0.5611 - val_loss: 0.4692 - val_categorical_accuracy: 0.6610\n",
      "3428/3428 [==============================] - 0s 48us/step\n",
      "Test loss: 0.8826048737228821    Test accuracy: 0.07117852975495916\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 1s 149us/step - loss: 0.6035 - categorical_accuracy: 0.7025 - val_loss: 0.4416 - val_categorical_accuracy: 0.7956\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 87us/step - loss: 0.4355 - categorical_accuracy: 0.7833 - val_loss: 0.3894 - val_categorical_accuracy: 0.8071\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 96us/step - loss: 0.3892 - categorical_accuracy: 0.8096 - val_loss: 0.3535 - val_categorical_accuracy: 0.8365\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 90us/step - loss: 0.3558 - categorical_accuracy: 0.8334 - val_loss: 0.3275 - val_categorical_accuracy: 0.8563\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 85us/step - loss: 0.3307 - categorical_accuracy: 0.8495 - val_loss: 0.2832 - val_categorical_accuracy: 0.8761\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 89us/step - loss: 0.2748 - categorical_accuracy: 0.8823 - val_loss: 0.2159 - val_categorical_accuracy: 0.9219\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 85us/step - loss: 0.2104 - categorical_accuracy: 0.9251 - val_loss: 0.1637 - val_categorical_accuracy: 0.9392\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 86us/step - loss: 0.1666 - categorical_accuracy: 0.9398 - val_loss: 0.1128 - val_categorical_accuracy: 0.9778\n",
      "3428/3428 [==============================] - 0s 38us/step\n",
      "Test loss: 0.2475968807796614    Test accuracy: 0.9366977829638273\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 295us/step - loss: 0.3810 - categorical_accuracy: 0.8198 - val_loss: 0.2276 - val_categorical_accuracy: 0.9113\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 238us/step - loss: 0.1860 - categorical_accuracy: 0.9252 - val_loss: 0.2129 - val_categorical_accuracy: 0.9103\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 226us/step - loss: 0.1070 - categorical_accuracy: 0.9619 - val_loss: 0.1220 - val_categorical_accuracy: 0.9556\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 3s 311us/step - loss: 0.0866 - categorical_accuracy: 0.9677 - val_loss: 0.0723 - val_categorical_accuracy: 0.9759\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 287us/step - loss: 0.0655 - categorical_accuracy: 0.9775 - val_loss: 0.0728 - val_categorical_accuracy: 0.9754\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 295us/step - loss: 0.0536 - categorical_accuracy: 0.9801 - val_loss: 0.0463 - val_categorical_accuracy: 0.9788\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 296us/step - loss: 0.0563 - categorical_accuracy: 0.9788 - val_loss: 0.0635 - val_categorical_accuracy: 0.9749\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 259us/step - loss: 0.0429 - categorical_accuracy: 0.9834 - val_loss: 0.0214 - val_categorical_accuracy: 0.9961\n",
      "3428/3428 [==============================] - 0s 50us/step\n",
      "Test loss: 0.15474355224903855    Test accuracy: 0.9641190198366394\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 195us/step - loss: 0.6237 - categorical_accuracy: 0.7034 - val_loss: 0.4320 - val_categorical_accuracy: 0.7792\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 97us/step - loss: 0.4278 - categorical_accuracy: 0.7913 - val_loss: 0.3770 - val_categorical_accuracy: 0.8014\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 103us/step - loss: 0.3780 - categorical_accuracy: 0.8190 - val_loss: 0.3359 - val_categorical_accuracy: 0.8332\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 104us/step - loss: 0.3388 - categorical_accuracy: 0.8469 - val_loss: 0.2795 - val_categorical_accuracy: 0.8891\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 97us/step - loss: 0.2950 - categorical_accuracy: 0.8717 - val_loss: 0.2353 - val_categorical_accuracy: 0.9026\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 97us/step - loss: 0.2422 - categorical_accuracy: 0.9022 - val_loss: 0.1912 - val_categorical_accuracy: 0.9291\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 97us/step - loss: 0.2019 - categorical_accuracy: 0.9216 - val_loss: 0.1455 - val_categorical_accuracy: 0.9576\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 96us/step - loss: 0.1719 - categorical_accuracy: 0.9379 - val_loss: 0.1154 - val_categorical_accuracy: 0.9677\n",
      "3428/3428 [==============================] - 0s 40us/step\n",
      "Test loss: 0.22775731881691388    Test accuracy: 0.9375729288214703\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 231us/step - loss: 0.3815 - categorical_accuracy: 0.8214 - val_loss: 0.2647 - val_categorical_accuracy: 0.8910\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 157us/step - loss: 0.1866 - categorical_accuracy: 0.9244 - val_loss: 0.1009 - val_categorical_accuracy: 0.9585\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 186us/step - loss: 0.1073 - categorical_accuracy: 0.9647 - val_loss: 0.0625 - val_categorical_accuracy: 0.9826\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 190us/step - loss: 0.0817 - categorical_accuracy: 0.9705 - val_loss: 0.0882 - val_categorical_accuracy: 0.9672\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 189us/step - loss: 0.0595 - categorical_accuracy: 0.9790 - val_loss: 0.0388 - val_categorical_accuracy: 0.9932\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 197us/step - loss: 0.0487 - categorical_accuracy: 0.9840 - val_loss: 0.0790 - val_categorical_accuracy: 0.9672\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 192us/step - loss: 0.0562 - categorical_accuracy: 0.9801 - val_loss: 0.0411 - val_categorical_accuracy: 0.9908\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 188us/step - loss: 0.0599 - categorical_accuracy: 0.9811 - val_loss: 0.0289 - val_categorical_accuracy: 0.9865\n",
      "3428/3428 [==============================] - 0s 77us/step\n",
      "Test loss: 0.18348882666434724    Test accuracy: 0.9542007001166861\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 182us/step - loss: 0.9296 - categorical_accuracy: 0.5300 - val_loss: 0.6237 - val_categorical_accuracy: 0.6933\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 99us/step - loss: 0.6499 - categorical_accuracy: 0.6344 - val_loss: 0.5080 - val_categorical_accuracy: 0.7420\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8293/8293 [==============================] - 1s 95us/step - loss: 0.5882 - categorical_accuracy: 0.6618 - val_loss: 0.4789 - val_categorical_accuracy: 0.7994\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 122us/step - loss: 0.5668 - categorical_accuracy: 0.6849 - val_loss: 0.4619 - val_categorical_accuracy: 0.8038\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 79us/step - loss: nan - categorical_accuracy: 0.6530 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 79us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 80us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 99us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 35us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 217us/step - loss: nan - categorical_accuracy: 0.5057 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 125us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 116us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 115us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 120us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 118us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 113us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 114us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 40us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 291us/step - loss: nan - categorical_accuracy: 0.4052 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 197us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 197us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 197us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 198us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 195us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 195us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 251us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 49us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 206us/step - loss: 0.8669 - categorical_accuracy: 0.5368 - val_loss: 0.5339 - val_categorical_accuracy: 0.7898\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 104us/step - loss: 0.6669 - categorical_accuracy: 0.6212 - val_loss: 0.4844 - val_categorical_accuracy: 0.6958\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 104us/step - loss: 0.6160 - categorical_accuracy: 0.6397 - val_loss: 0.4638 - val_categorical_accuracy: 0.6972\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 94us/step - loss: 0.5871 - categorical_accuracy: 0.6530 - val_loss: 0.4566 - val_categorical_accuracy: 0.6856\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 110us/step - loss: 0.5648 - categorical_accuracy: 0.6685 - val_loss: 0.4338 - val_categorical_accuracy: 0.7290\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 108us/step - loss: 0.5557 - categorical_accuracy: 0.6859 - val_loss: 0.4134 - val_categorical_accuracy: 0.7459\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 114us/step - loss: 0.5354 - categorical_accuracy: 0.7145 - val_loss: 0.3670 - val_categorical_accuracy: 0.8602\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 112us/step - loss: 0.5156 - categorical_accuracy: 0.7386 - val_loss: 0.3532 - val_categorical_accuracy: 0.8679\n",
      "3428/3428 [==============================] - 0s 44us/step\n",
      "Test loss: 0.5199694706511748    Test accuracy: 0.8051341890315052\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 3s 397us/step - loss: 0.3828 - categorical_accuracy: 0.8164 - val_loss: 0.2702 - val_categorical_accuracy: 0.8833\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 297us/step - loss: nan - categorical_accuracy: 0.4362 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 301us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 3s 304us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 298us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 301us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 3s 339us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 272us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 57us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 206us/step - loss: 0.6365 - categorical_accuracy: 0.6791 - val_loss: 0.4361 - val_categorical_accuracy: 0.7922\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 94us/step - loss: 0.4529 - categorical_accuracy: 0.7658 - val_loss: 0.3853 - val_categorical_accuracy: 0.8028\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 96us/step - loss: 0.4085 - categorical_accuracy: 0.7973 - val_loss: 0.3669 - val_categorical_accuracy: 0.8298\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 94us/step - loss: 0.3833 - categorical_accuracy: 0.8153 - val_loss: 0.3484 - val_categorical_accuracy: 0.8240\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 93us/step - loss: 0.3693 - categorical_accuracy: 0.8224 - val_loss: 0.3378 - val_categorical_accuracy: 0.8505\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 97us/step - loss: 0.3548 - categorical_accuracy: 0.8344 - val_loss: 0.3194 - val_categorical_accuracy: 0.8549\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 95us/step - loss: 0.3356 - categorical_accuracy: 0.8429 - val_loss: 0.2993 - val_categorical_accuracy: 0.8732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 96us/step - loss: 0.3229 - categorical_accuracy: 0.8508 - val_loss: 0.2877 - val_categorical_accuracy: 0.8785\n",
      "3428/3428 [==============================] - 0s 42us/step\n",
      "Test loss: 0.4487792512569294    Test accuracy: 0.8010501750291715\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 241us/step - loss: nan - categorical_accuracy: 0.7219 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 154us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 139us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 131us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 151us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 146us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 137us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 133us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 52us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 3s 412us/step - loss: 0.3703 - categorical_accuracy: 0.8283 - val_loss: 0.2472 - val_categorical_accuracy: 0.9007\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 301us/step - loss: 0.1885 - categorical_accuracy: 0.9239 - val_loss: 0.1295 - val_categorical_accuracy: 0.9556\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 254us/step - loss: 0.1063 - categorical_accuracy: 0.9635 - val_loss: 0.0890 - val_categorical_accuracy: 0.9841\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 229us/step - loss: 0.1053 - categorical_accuracy: 0.9638 - val_loss: 0.0629 - val_categorical_accuracy: 0.9769\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 234us/step - loss: 0.0608 - categorical_accuracy: 0.9779 - val_loss: 0.0650 - val_categorical_accuracy: 0.9749\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 242us/step - loss: 0.0560 - categorical_accuracy: 0.9779 - val_loss: 0.1407 - val_categorical_accuracy: 0.9470\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 249us/step - loss: 0.0539 - categorical_accuracy: 0.9807 - val_loss: 0.0319 - val_categorical_accuracy: 0.9904\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 237us/step - loss: 0.0489 - categorical_accuracy: 0.9840 - val_loss: 0.0213 - val_categorical_accuracy: 0.9928\n",
      "3428/3428 [==============================] - 0s 58us/step\n",
      "Test loss: 0.17204399829373945    Test accuracy: 0.9609101516919487\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 239us/step - loss: nan - categorical_accuracy: 0.5285 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 115us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 111us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 110us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 111us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 126us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 111us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 117us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 52us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 235us/step - loss: nan - categorical_accuracy: 0.4060 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 112us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 110us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 110us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 126us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 118us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 116us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 108us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 69us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 270us/step - loss: 0.8764 - categorical_accuracy: 0.5618 - val_loss: 0.5335 - val_categorical_accuracy: 0.7305\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 111us/step - loss: 0.5872 - categorical_accuracy: 0.6840 - val_loss: 0.4551 - val_categorical_accuracy: 0.7985\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 85us/step - loss: 0.5224 - categorical_accuracy: 0.7392 - val_loss: 0.4067 - val_categorical_accuracy: 0.8115\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 87us/step - loss: 0.4858 - categorical_accuracy: 0.7563 - val_loss: 0.3790 - val_categorical_accuracy: 0.8346\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 85us/step - loss: 0.4537 - categorical_accuracy: 0.7851 - val_loss: 0.3501 - val_categorical_accuracy: 0.8500\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 105us/step - loss: 0.4260 - categorical_accuracy: 0.7979 - val_loss: 0.3258 - val_categorical_accuracy: 0.8664\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 93us/step - loss: 0.4086 - categorical_accuracy: 0.8054 - val_loss: 0.3120 - val_categorical_accuracy: 0.8698\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 87us/step - loss: nan - categorical_accuracy: 0.4096 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 42us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 3s 368us/step - loss: 0.3900 - categorical_accuracy: 0.8161 - val_loss: 0.2815 - val_categorical_accuracy: 0.8824\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 207us/step - loss: 0.2084 - categorical_accuracy: 0.9147 - val_loss: 0.1506 - val_categorical_accuracy: 0.9359\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8293/8293 [==============================] - 2s 259us/step - loss: 0.1103 - categorical_accuracy: 0.9603 - val_loss: 0.0682 - val_categorical_accuracy: 0.9740\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 261us/step - loss: 0.0809 - categorical_accuracy: 0.9701 - val_loss: 0.0625 - val_categorical_accuracy: 0.9687\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 214us/step - loss: 0.0776 - categorical_accuracy: 0.9734 - val_loss: 0.0563 - val_categorical_accuracy: 0.9754\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 235us/step - loss: 0.0489 - categorical_accuracy: 0.9813 - val_loss: 0.0510 - val_categorical_accuracy: 0.9812\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 204us/step - loss: 0.0461 - categorical_accuracy: 0.9838 - val_loss: 0.0610 - val_categorical_accuracy: 0.9802\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 254us/step - loss: 0.0370 - categorical_accuracy: 0.9876 - val_loss: 0.0320 - val_categorical_accuracy: 0.9846\n",
      "3428/3428 [==============================] - 0s 100us/step\n",
      "Test loss: 0.1628642130107646    Test accuracy: 0.9524504084014003\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 4s 465us/step - loss: nan - categorical_accuracy: 0.8037 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 185us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 131us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 185us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 185us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 185us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 193us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 171us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 41us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 2s 237us/step - loss: 0.6470 - categorical_accuracy: 0.6847 - val_loss: 0.4460 - val_categorical_accuracy: 0.7927\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 87us/step - loss: 0.4519 - categorical_accuracy: 0.7811 - val_loss: 0.3830 - val_categorical_accuracy: 0.8086\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 89us/step - loss: 0.4007 - categorical_accuracy: 0.8089 - val_loss: 0.3456 - val_categorical_accuracy: 0.8288\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 129us/step - loss: 0.3645 - categorical_accuracy: 0.8311 - val_loss: 0.3188 - val_categorical_accuracy: 0.8640\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 148us/step - loss: 0.3376 - categorical_accuracy: 0.8443 - val_loss: 0.3016 - val_categorical_accuracy: 0.8742\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 148us/step - loss: 0.3087 - categorical_accuracy: 0.8623 - val_loss: 0.2645 - val_categorical_accuracy: 0.8954\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 145us/step - loss: 0.2757 - categorical_accuracy: 0.8869 - val_loss: 0.2268 - val_categorical_accuracy: 0.9127\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 157us/step - loss: 0.2394 - categorical_accuracy: 0.9070 - val_loss: 0.1909 - val_categorical_accuracy: 0.9335\n",
      "3428/3428 [==============================] - 0s 67us/step\n",
      "Test loss: 0.3680924663326704    Test accuracy: 0.8728121353558926\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 3s 340us/step - loss: 0.6569 - categorical_accuracy: 0.6736 - val_loss: 0.4361 - val_categorical_accuracy: 0.8129\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 157us/step - loss: 0.4656 - categorical_accuracy: 0.7757 - val_loss: 0.3704 - val_categorical_accuracy: 0.8240\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 156us/step - loss: 0.4048 - categorical_accuracy: 0.8063 - val_loss: 0.3400 - val_categorical_accuracy: 0.8337\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 156us/step - loss: 0.3692 - categorical_accuracy: 0.8243 - val_loss: 0.3059 - val_categorical_accuracy: 0.8607\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 154us/step - loss: 0.3421 - categorical_accuracy: 0.8411 - val_loss: 0.2784 - val_categorical_accuracy: 0.8766\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 1s 155us/step - loss: nan - categorical_accuracy: 0.5374 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 1s 154us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 1s 161us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 74us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 4s 486us/step - loss: nan - categorical_accuracy: 0.4208 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 297us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 290us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 292us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 285us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 287us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 289us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 288us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 85us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 5s 578us/step - loss: 0.3699 - categorical_accuracy: 0.8272 - val_loss: 0.2511 - val_categorical_accuracy: 0.8934\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 3s 375us/step - loss: 0.1788 - categorical_accuracy: 0.9326 - val_loss: 0.1445 - val_categorical_accuracy: 0.9301\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 3s 393us/step - loss: 0.1185 - categorical_accuracy: 0.9555 - val_loss: 0.0785 - val_categorical_accuracy: 0.9667\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 3s 339us/step - loss: 0.0844 - categorical_accuracy: 0.9729 - val_loss: 0.1408 - val_categorical_accuracy: 0.9730\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 274us/step - loss: 0.0863 - categorical_accuracy: 0.9762 - val_loss: 0.0709 - val_categorical_accuracy: 0.9706\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 3s 365us/step - loss: 0.0671 - categorical_accuracy: 0.9770 - val_loss: 0.0381 - val_categorical_accuracy: 0.9870\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8293/8293 [==============================] - 2s 229us/step - loss: 0.0410 - categorical_accuracy: 0.9841 - val_loss: 0.0885 - val_categorical_accuracy: 0.9638\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 208us/step - loss: 0.0505 - categorical_accuracy: 0.9822 - val_loss: 0.0595 - val_categorical_accuracy: 0.9749\n",
      "3428/3428 [==============================] - 0s 53us/step\n",
      "Test loss: 0.1794013180683042    Test accuracy: 0.9448658109684948\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 4s 469us/step - loss: 0.3928 - categorical_accuracy: 0.8161 - val_loss: 0.3137 - val_categorical_accuracy: 0.8346\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 265us/step - loss: 0.2155 - categorical_accuracy: 0.9145 - val_loss: 0.1281 - val_categorical_accuracy: 0.9470\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 266us/step - loss: 0.1154 - categorical_accuracy: 0.9571 - val_loss: 0.0798 - val_categorical_accuracy: 0.9778\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 286us/step - loss: 0.0792 - categorical_accuracy: 0.9746 - val_loss: 0.0930 - val_categorical_accuracy: 0.9716\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 277us/step - loss: 0.0727 - categorical_accuracy: 0.9743 - val_loss: 0.0449 - val_categorical_accuracy: 0.9860\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 271us/step - loss: 0.0562 - categorical_accuracy: 0.9813 - val_loss: 0.0357 - val_categorical_accuracy: 0.9879\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 270us/step - loss: 0.0542 - categorical_accuracy: 0.9829 - val_loss: 0.0543 - val_categorical_accuracy: 0.9759\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 270us/step - loss: 0.0475 - categorical_accuracy: 0.9835 - val_loss: 0.0354 - val_categorical_accuracy: 0.9841\n",
      "3428/3428 [==============================] - 0s 87us/step\n",
      "Test loss: 0.16291251585057034    Test accuracy: 0.9585764294049008\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 3s 371us/step - loss: 0.3849 - categorical_accuracy: 0.8236 - val_loss: 0.2690 - val_categorical_accuracy: 0.8809\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 176us/step - loss: 0.2018 - categorical_accuracy: 0.9231 - val_loss: 0.1225 - val_categorical_accuracy: 0.9513\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 195us/step - loss: nan - categorical_accuracy: 0.6702 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 233us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 266us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 269us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 272us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 250us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 89us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 5s 628us/step - loss: nan - categorical_accuracy: 0.3810 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 3s 311us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 3s 378us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 3s 398us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 3s 379us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 3s 382us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 3s 384us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 3s 407us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 103us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 4s 427us/step - loss: nan - categorical_accuracy: 0.5017 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 2s 192us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 2s 189us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 2s 193us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 2s 184us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 186us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 191us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 205us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 92us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "8293/8293 [==============================] - 3s 403us/step - loss: 0.5274 - categorical_accuracy: 0.7495 - val_loss: 0.3853 - val_categorical_accuracy: 0.8192\n",
      "Epoch 2/8\n",
      "8293/8293 [==============================] - 1s 113us/step - loss: 0.3861 - categorical_accuracy: 0.8182 - val_loss: 0.3200 - val_categorical_accuracy: 0.8549\n",
      "Epoch 3/8\n",
      "8293/8293 [==============================] - 1s 120us/step - loss: 0.3252 - categorical_accuracy: 0.8541 - val_loss: 0.2493 - val_categorical_accuracy: 0.8983\n",
      "Epoch 4/8\n",
      "8293/8293 [==============================] - 1s 120us/step - loss: 0.2515 - categorical_accuracy: 0.8973 - val_loss: 0.1538 - val_categorical_accuracy: 0.9489\n",
      "Epoch 5/8\n",
      "8293/8293 [==============================] - 1s 120us/step - loss: 0.1753 - categorical_accuracy: 0.9350 - val_loss: 0.0888 - val_categorical_accuracy: 0.9802\n",
      "Epoch 6/8\n",
      "8293/8293 [==============================] - 2s 183us/step - loss: nan - categorical_accuracy: 0.5215 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 7/8\n",
      "8293/8293 [==============================] - 2s 216us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "Epoch 8/8\n",
      "8293/8293 [==============================] - 2s 202us/step - loss: nan - categorical_accuracy: 0.3300 - val_loss: nan - val_categorical_accuracy: 0.3394\n",
      "3428/3428 [==============================] - 0s 84us/step\n",
      "Test loss: nan    Test accuracy: 0.021295215869311553\n",
      "Train on 8293 samples, validate on 2074 samples\n",
      "Epoch 1/8\n",
      "5824/8293 [====================>.........] - ETA: 1s - loss: 0.4253 - categorical_accuracy: 0.7967"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results_gp = skopt.gp_minimize(objective,     # the function to minimize\n",
    "                          SPACE,              # the bounds on each dimension of x\n",
    "                          x0=[100,50,0],      # the starting point\n",
    "                          acq_func=\"LCB\",     # the acquisition function (optional)\n",
    "                          n_calls=50,         # the number of evaluations of f including at x0\n",
    "                          n_random_starts=4,  # the number of random initialization points\n",
    "                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Best Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score=%.4f\" % results_gp.fun)\n",
    "\n",
    "print(\"\"\"Best parameters:\n",
    "- dense_0_neurons=%d\n",
    "- dense_1_neurons=%d\n",
    "- dropout_rate=%.6f\"\"\" % \n",
    "      (results_gp.x[0], results_gp.x[1],\n",
    "                            results_gp.x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(results_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations Plot\n",
    "plots.plot_evaluations creates a grid of size n_dims by n_dims. The diagonal shows histograms for each of the dimensions. In the lower triangle (just one plot in this case) a two dimensional scatter plot of all points is shown. The order in which points were evaluated is encoded in the color of each point. Darker/purple colors correspond to earlier samples and lighter/yellow colors correspond to later samples. A red point shows the location of the minimum found by the optimization process.\n",
    "\n",
    "You should be able to see that points start clustering around the location of the true miminum. The histograms show that the objective is evaluated more often at locations near to one of the three minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_evaluations\n",
    "_ = plot_evaluations(results_gp, bins=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Partial Dependence Plots\n",
    "Partial dependence plots were proposed by [Friedman (2001)] as a method for interpreting the importance of input features used in gradient boosting machines. The idea is to visualize how the value of $i$-th variable $x_i$ influences the function $f$ after averaging out the influence of all other variables.For more details see: https://christophm.github.io/interpretable-ml-book/pdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "_ = plot_objective(results_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Further using Best Hyper-parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using a specified hyperparameters.\n",
    "\n",
    "# Get best hyper-parameters\n",
    "dense_0_neurons=results_gp.x[0]; dense_1_neurons=results_gp.x[1]; dropout_rate=results_gp.x[2]\n",
    "\n",
    "# Build \n",
    "model = get_model(dense_0_neurons, dense_1_neurons, dropout_rate, num_features, num_classes)\n",
    "\n",
    "# Compile the keras model for a specified number of epochs.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "# Fit keras model\n",
    "history = model.fit(X_train, y_train_one_hot, epochs=16, batch_size=16, \n",
    "                        validation_split = 0.20, verbose=1)\n",
    "\n",
    "# Evaluate the model with the eval dataset.\n",
    "score = model.evaluate(X_test, y_test_one_hot,\n",
    "                                  batch_size=16, verbose=0)\n",
    "print('Test loss:', score[0], '   Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Training History\n",
    "In this section, we will produce plots of your model's accuracy and loss on the training and validation set. These are useful to check for overfitting. Additionally, you can produce these plots for any of the metrics you created above. False negatives are included as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for plotting history\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'categorical_accuracy']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(1,2,n+1)\n",
    "    plt.tight_layout()\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training/validation history of our Keras model\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Metrics\n",
    "You can use a confusion matrix to summarize the actual vs. predicted labels where the X axis is the predicted label and the Y axis is the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_cm(labels, predictions):\n",
    "  cm = confusion_matrix(labels, predictions)\n",
    "  plt.figure(figsize=(8,8))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model on the test dataset and display the evaluation metrics and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = model.predict_classes(X_test, batch_size=4)\n",
    "baseline_results = model.evaluate(X_test, y_test_one_hot,\n",
    "                                  batch_size=4, verbose=0)\n",
    "\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(Y_test, y_test_predictions+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see we have successfully trained a Multi-Layer perceptron which was written in Keras using TensorFlow backend with high validation accuracy!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
